{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e39565f-01fe-4d64-ad1a-83d69e597052",
   "metadata": {},
   "source": [
    "# Joint Intent detection and slot filling\n",
    "Dieses Jupyter notebook wurde als semesterabschließende Arbeit für das Modul Natural Language Processing an der [Fachhochschule Südwestfalen](https://www.fh-swf.de/en/international_3/index.php) erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e89dc9-708f-4c4f-8371-36f810242e7d",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "Das Joint intent detection and slot filling (IDSF) ist eine Aufgabe aus dem Teilbereich des Natural Language Understandings (NLU) des Natural Language Processings (NLP), die uns in heutzutage fast täglich Alltag begegnet. Sei es um einen Timer auf dem Handy zu starten, bestimmte Musik abzuspielen oder das Licht einzuschalten. Der Ablauf ist dabei häufig der selbe: \"Siri stelle einen Timer für 4 Minuten\", \"Alexa spiele meine Schlager Playlist\" oder \"Google erstelle einen Arzttermin für heute 16:00 Uhr\". Meist beginnen die Kommandos mit dem Namen des Sprachassistenten, um diesen zu aktivieren, gefolgt vom Kommando für die gewünschte Aktion. Das IDSF beschäftigt sich dabei mit der Aufgabe, die gewünschte Aktion (Intent), also stelle einen Timer, Spiele Musik, erstelle einen Termin im Kalender und die dazugehörigen notwendigen Parameter, wie z.B. vier Minuten, Schlager Playlist oder Arzt heute 16:00 Uhr (Slots) zu erkennen.\n",
    "Da der Gebrauch dieser Sprachassistenten in Zukunft wahrscheinlich noch stärker zu nehmen wird, wollen wir uns deren funktionsweise in diesem Notebook näher anschauen. Dafür wird zuerst die Entwicklungshistorie vom IDSF betrachtet und anschließend wird ein eigenes Modell für die Erkennung erstellt und anhand eines selbst vorbereiteten Korpus trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d088a2-b7f1-41f3-bd3b-1858acb800e7",
   "metadata": {},
   "source": [
    "## Joint Intent Detection and Slot filling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bb627-85be-4f27-b800-9860a15cb12f",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f3c76-2bf2-49ff-9c87-6a1841c2c9ef",
   "metadata": {},
   "source": [
    "## Datenbeschaffung\n",
    "\n",
    "Als Datensatz für das nachfolgende Beispiel verwenden wir den Snips-Datensatz. Dieser Datensatz wurde vom, mittlerweil zu Sonos gehörenden [1], [Snips Team](https://snips.ai/) zusammengestellt, um ihr eigenes Modell mit anderen Wettbewerbern wie zum Beispiel Amazons Alexa zu verglichen. Die Ergebnisse und die Datensätze der drei Vergleiche wurden in einem [GitHub Repository](https://github.com/sonos/nlu-benchmark/tree/master) veröffentlicht und in dem Paper \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces\" [2] erläutert. Das Repository enthält die Daten für drei Evaluationen aus den Jahren 2016 bis 2018. Wir werden in diesem Notebook die Daten der 2017 durchgeführten Evaluation verwenden, da diese Sätze für sieben unterschiedliche und allgemeine Aufgaben enthält.\n",
    "\n",
    "Die Daten sind im dem Repository in einzelnen JSON-Dateien enthalten. Dabei gibt es pro Aufgabe zwei Dateien, eine für das Training und eine für die Validierung. Der Einfachheit halber wurden die Dateien in dem data Verzeichnis, dass diesem Notebook beiliegt, abgelegt.\n",
    "\n",
    "Nachfolgend ist ein Auszug aus der `train_AddToPlaylist_full.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04077bd-4c1c-475b-bd64-997f7afd2ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"AddToPlaylist\": [\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"Add another \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"song\",\n",
      "          \"entity\": \"music_item\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" to the \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Cita Romántica\",\n",
      "          \"entity\": \"playlist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"add \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"clem burke\",\n",
      "          \"entity\": \"artist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" in \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"my\",\n",
      "          \"entity\": \"playlist_owner\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Pre-Party R&B Jams\",\n",
      "          \"entity\": \"playlist\"\n",
      "        }\n",
      "      ]\n",
      "    },\n"
     ]
    }
   ],
   "source": [
    "!head -n 48 data/train_AddToPlaylist_full.json # Zeige die ersten 23 Zeilen der angegebenen Datei an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7977d9c-5cc6-4725-9386-46716911c4f5",
   "metadata": {},
   "source": [
    "Die Datei besteht an oberster Stelle aus dem Namen der gewünschten Aktion gefolgt von einer Liste an Objekten mit einem `Data` Attribut. Dieses enthält wiederum eine Liste von Objekten mit `Text` Attributen die den Satz in Teilen enthält. Dabei wird der Satz durch den Text eines definierten `Entities` geteilt. So enthält das erste Beispiel den Text bis zum ersten `entity` das als `music_item` klassifiziert wurde und wieder den gesamten Text bis zum nächsten entity, dem Namen einer Playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5966d-61dc-45af-8107-2fbf9f55b2de",
   "metadata": {},
   "source": [
    "Als nächstes werden die Daten in ein verwendbares Format transformiert. Ein in der Natural language processing gängiges Format ist das IOB Format. IOB steht für Inside-Outside-Beginning. Dieses Format ermöglicht die Kennzeichnung der einzelnen Entitäten in einem Satz. Es wird unteranderem von den weit verbreiteten Python Bibliotheken `NLTK` und `spaCy` unterstützt [3, 4]. Das Format wurde 1995 von Lance A. Ramshaw und Mitchell P. Marcus erfunden.\n",
    "\n",
    "Dieses Beispiel zeigt das Format einer Zeile, welches nachfolgend aus den JSON-Dateien erzeugt wird.\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o i-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "Am Beginn der Zeile steht der vollständige Satz abgetrennt durch ein BOS (begin of sentence) am Anfang des Satzes und ein EOS (end of sentence) am Ende des Satzes. Nun folgt das eigentliche IOB-Format. Dabei wird für jeden Token entweder der Buchstabe 'o', dieser steht für keine Bedeutung, der Buchstabe 'b', für den Beginn einer Entität die aus mehreren Token besteht, oder 'i', als Entität. Das 'i' steht dabei entweder nach einem 'b' wodurch eine Entität gekennzeichent wird, die aus mehreren Token besteht oder alleine für eine Entität die aus nur einem Token besteht.  Das 'b' und 'i' werden dabei jeweils gefolgt vom einem trennenden Bindestrich und der Entitätskategorie verwender. So ist der Name 'Clem Burke' unterteilt in ein `b-music_item` für Clem und `i-Music_item` für Burke. Dadurch wird definiert, dass die beiden Teile zusammen gehören.\n",
    "\n",
    "Das IOB2 Format ist eine Erweiterung des originalen IOB Formats. Es definiert das auch eine Entität die nur aus einem Token besteht mit einem 'b' kodiert wird und nicht wie im IOB Format mit einem 'i'. Dadurch ergibt sich das folgende Format:\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o b-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "\n",
    "Mit dem folgenden Python Code wird der Inhalt der im data-Verzeichnis liegenden Dateien in das vorgestellte Format transformiert. Die Dateien werden dabei im Verzeichnis `data/corpus` und einem Verzeichnis mit dem Titel der Intent-Kategorie abgelegt. Als Dateinamen werden `UUID`s verwendet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23b37ed-4ed4-4f10-b47f-d9856cac14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from shutil import rmtree\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049daa05-aa7a-4076-b36a-f4deb1ee1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alte Corpus-Dateien gelöscht\n",
      "Das Pfad data existiert nicht oder ist kein Verzeichnis.\n",
      "Try converting file at path data/validate_PlayMusic.json\n",
      "Finished converting file at path data/validate_PlayMusic.json. Writing to file...\n",
      "Try converting file at path data/train_SearchCreativeWork_full.json\n",
      "Finished converting file at path data/train_SearchCreativeWork_full.json. Writing to file...\n",
      "Try converting file at path data/train_AddToPlaylist_full.json\n",
      "Finished converting file at path data/train_AddToPlaylist_full.json. Writing to file...\n",
      "Try converting file at path data/train_RateBook_full.json\n",
      "Finished converting file at path data/train_RateBook_full.json. Writing to file...\n",
      "Try converting file at path data/train_SearchScreeningEvent_full.json\n",
      "Finished converting file at path data/train_SearchScreeningEvent_full.json. Writing to file...\n",
      "Try converting file at path data/validate_GetWeather.json\n",
      "Finished converting file at path data/validate_GetWeather.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchCreativeWork.json\n",
      "Finished converting file at path data/validate_SearchCreativeWork.json. Writing to file...\n",
      "Try converting file at path data/train_GetWeather_full.json\n",
      "Finished converting file at path data/train_GetWeather_full.json. Writing to file...\n",
      "Try converting file at path data/train_PlayMusic_full.json\n",
      "Finished converting file at path data/train_PlayMusic_full.json. Writing to file...\n",
      "Try converting file at path data/validate_RateBook.json\n",
      "Finished converting file at path data/validate_RateBook.json. Writing to file...\n",
      "Try converting file at path data/validate_AddToPlaylist.json\n",
      "Finished converting file at path data/validate_AddToPlaylist.json. Writing to file...\n",
      "Try converting file at path data/validate_BookRestaurant.json\n",
      "Finished converting file at path data/validate_BookRestaurant.json. Writing to file...\n",
      "Try converting file at path data/train_BookRestaurant_full.json\n",
      "Finished converting file at path data/train_BookRestaurant_full.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchScreeningEvent.json\n",
      "Finished converting file at path data/validate_SearchScreeningEvent.json. Writing to file...\n",
      "Finished converting all files!\n"
     ]
    }
   ],
   "source": [
    "def clean_formatted(formatted_directory_path):\n",
    "    if not os.path.exists(formatted_directory_path) or not os.path.isdir(formatted_directory_path):\n",
    "        print(f'Pfad {formatted_directory_path} ist kein Verzeichnis oder existiert nicht')\n",
    "        return\n",
    "\n",
    "    rmtree(formatted_directory_path)\n",
    "    print('Alte Corpus-Dateien gelöscht')\n",
    "\n",
    "def convert_file(json_file_path, corpus_root):\n",
    "    print(f'Try converting file at path {json_file_path}')\n",
    "    if not os.path.isfile(json_file_path):\n",
    "        print(f'File {file_path} does not exists', json_file_path)\n",
    "        return \n",
    "    formatted_lines = []\n",
    "    intent_category = None\n",
    "    all_slots_set = set()\n",
    "    all_slots_set.add('o')\n",
    "    with open(json_file_path, 'r', encoding='latin-1') as json_file:\n",
    "        json_content = json.load(json_file)\n",
    "        intent_category = next(iter(json_content))\n",
    "        for sentence_block in json_content[intent_category]:\n",
    "            sentence = \"\"\n",
    "            slots = []\n",
    "            for sentence_data_block in sentence_block['data']:\n",
    "                sentence_part = sentence_data_block['text']\n",
    "                sentence_part = re.sub('\\n', '', sentence_part)\n",
    "                if sentence_part != '':\n",
    "                    sentence += sentence_part\n",
    "                sentence_part_len = len(sentence_part.split())\n",
    "                if 'entity' in sentence_data_block:\n",
    "                    entity_type = sentence_data_block['entity']\n",
    "                    if sentence_part_len > 1:\n",
    "                        firstSlot = True\n",
    "                        for i in range(sentence_part_len):\n",
    "                            if firstSlot:\n",
    "                                slots.append('b-' + entity_type)\n",
    "                                firstSlot = False\n",
    "                                all_slots_set.add('b-' + entity_type)\n",
    "                            else:\n",
    "                                slots.append('i-' + entity_type)\n",
    "                                all_slots_set.add('i-' + entity_type)\n",
    "                    else:\n",
    "                        slots.append('b-' + entity_type)\n",
    "                        all_slots_set.add('b-' + entity_type)\n",
    "                else:\n",
    "                    for i in range(sentence_part_len):\n",
    "                        slots.append('o')\n",
    "            formatted_lines.append(construct_row(sentence, intent_category, slots))\n",
    "    print(f'Finished converting file at path {json_file_path}. Writing to file...')\n",
    "    write_to_file(intent_category, formatted_lines, corpus_root)\n",
    "    return all_slots_set\n",
    "    \n",
    "\n",
    "def construct_row(sentence, intent, slots):\n",
    "    row = ''\n",
    "    row += sentence\n",
    "    row += '#!#'\n",
    "    row += intent\n",
    "    row += '#!#'\n",
    "    row += ' '.join(slots)\n",
    "    row += '\\n'\n",
    "    return row\n",
    "\n",
    "\n",
    "def write_to_file(intent, lines, corpus_root):\n",
    "    if intent is None or intent == '':\n",
    "        print('No intent')\n",
    "        return\n",
    "        \n",
    "    base_output_directory = corpus_root\n",
    "    output_file_path = base_output_directory + intent + '/' + str(uuid.uuid4()) + '.txt'\n",
    "\n",
    "    if not os.path.exists(base_output_directory + intent): \n",
    "        os.makedirs(base_output_directory + intent)\n",
    "    \n",
    "    with open(output_file_path, 'a') as output_file:\n",
    "        output_file.writelines(lines)\n",
    "\n",
    "\n",
    "def write_slots_to_file(corpus_root, slots):\n",
    "    with open(corpus_root + 'slots.txt', 'w') as slots_file:\n",
    "        lines = [slot + \"\\n\" for slot in slots]\n",
    "        slots_file.writelines(lines)\n",
    "\n",
    "\n",
    "def iterate_over_json_files_in_directory(directory_path, corpus_root):\n",
    "    if not os.path.exists(directory_path) or directory_path is not os.path.isdir(directory_path):\n",
    "        print(f\"Das Pfad {directory_path} existiert nicht oder ist kein Verzeichnis.\")\n",
    "\n",
    "    slots = set()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):  # Nur JSON-Dateien berücksichtigen\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            slots_from_file = convert_file(file_path, corpus_root)\n",
    "        slots.update(slots_from_file)\n",
    "\n",
    "    write_slots_to_file(corpus_root, slots)\n",
    "    print('Finished converting all files!')\n",
    "    \n",
    "    \n",
    "clean_formatted('data/corpus')\n",
    "iterate_over_json_files_in_directory('data', 'data/corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dd92-a1b1-404d-b1af-e13fa5ebbfa4",
   "metadata": {},
   "source": [
    "Als nächstes wird geprüft, ob auch alle Einträge in der Textdatei enthalten sind. Dafür werden die Einträge in der train- und validate.json mit der Anzahl der Zeilen in der Textdatei verglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99108436-e37e-4954-be8d-66a2e8104be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2056 data/formatted/RateBook.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "579d5d3c-fc2a-490c-98fe-c98bf612674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m1956\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/train_RateBook_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431915fc-2450-4617-aad4-37ecd6e94991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/validate_RateBook.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490f728-7789-4273-902a-8f9f11235122",
   "metadata": {},
   "source": [
    "Man sieht, dass die Anzahl der `data`-Blöcke aus den JSON-Dateien der Anzahl der Zeilen in der erzeugten Textdatei entspricht. Die Konvertierung war also erfolgreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32f47ef-983c-4ba5-a359-a808e2cf2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS rate The Lotus and the Storm zero of 6 EOS RateBook o b-object_name i-object_name i-object_name i-object_name i-object_name b-rating_value o b-best_rating\n",
      "BOS Rate The Fall-Down Artist 5 stars. EOS RateBook o b-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS Rate the current novel one points EOS RateBook o o b-object_select b-object_type b-rating_value b-rating_unit\n",
      "BOS rate The Ape-Man Within 4 EOS RateBook o b-object_name i-object_name i-object_name b-rating_value\n",
      "BOS I give The Penalty three stars EOS RateBook o o b-object_name i-object_name b-rating_value b-rating_unit\n",
      "BOS rate this novel a 4 EOS RateBook o b-object_select b-object_type o b-rating_value\n",
      "BOS give 5 out of 6 points to Absolutely, Positively Not series EOS RateBook o b-rating_value o o b-best_rating b-rating_unit o b-object_name i-object_name i-object_name b-object_part_of_series_type\n",
      "BOS I give Emile, or On Education five points. EOS RateBook o o b-object_name i-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS rate Licence Renewed a 4 EOS RateBook o b-object_name i-object_name o b-rating_value\n",
      "BOS Give this essay a 2 out of 6. EOS RateBook o b-object_select b-object_type o b-rating_value o o b-best_rating o\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9ff63-9acc-462e-b20b-c39fcb78caf4",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 1: https://investors.sonos.com/news-and-events/investor-news/latest-news/2019/Sonos-Announces-Acquisition-of-Snips/default.aspx, [Online, 07.03.2025]\n",
    "* 2: Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.\" 2018, [Online: https://arxiv.org/abs/1805.10190, 07.03.2025]\n",
    "* 3: NLTK Team, tree2conlltags, [Online: https://www.nltk.org/_modules/nltk/chunk/util.html#tree2conlltags, 13.03.2025]\n",
    "* 4: Explosion, spaCy convert, [Online: https://spacy.io/api/cli#converters, 13.03.2025]\n",
    "* 5: Ramshaw und Marcus, \"Text Chunking using Transformation-Based Learning\" 1995, [Online: https://arxiv.org/abs/cmp-lg/9505040, 14.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511baabd-4337-4292-9483-482fd44392e8",
   "metadata": {},
   "source": [
    "## Erstellen eines Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc2109-e2af-4849-a245-6867a42bea03",
   "metadata": {},
   "source": [
    "Nachdem nun die Daten in ein verwendbares Format tranformiert wurden, müssen wir den effiziente Zugriff auf die Daten sicherstellen. Dafür bietet die Python-Bibliothek NLTK verschiedene `CorpusReader` Klassen zur Verfügung. Diese Klassen ermöglichen über Methoden den Zugriff auf die Dokumente selbst, sowie auf weitere Dateien eines Corpus, wie zum Beispiel die Lizenz des Corpus oder eine README.md Datei. NLTK bietet eine große Anzahl an CorpusReadern für verschiedenste Szenarien an. Ein einfacher CorpusReader ist der `PlainTextCorpusReader` [7]. Dieser bietet Zugriff auf reine Textdateien. Eine vollständige Liste ist in der [Dokumentation](https://www.nltk.org/api/nltk.corpus.reader.html) von NLTK zu finden.\n",
    "\n",
    "Für das zuvor definierte Format gibt es keinen passenden vorgefertigten Reader. Daher müssen wir einen eigenen erstellen. Dafür kann die `CorpusReader`-Basisklasse [8] erweitert werden oder ein bestehender verwendet werden. Wir werden den `CategorizedPlaintextCorpusReader` als Basis verwenden. Dieser ist eine Erweiterung des zuvor erwähnten `PlainTextCorpusReader`, welcher Zugriff auf Textdateien bietet.  Durch die Nutzung des `CategorizedPlaintextCorpusReader` können die Dateien zusätzlich in Kategorien unterteilt werden. Dies geschieht über reguläre Ausdrücke. Im nachfolgenden Code definiert der reguläre Ausdruck _category_pattern_ die Kategorie als den Namen des Verzeichnisses, indem die Textdateien liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c56201-542f-4d46-8644-27e5da9e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from nltk import wordpunct_tokenize\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe06789a-d306-4896-a9f9-0640f8d0d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOBCorpusReader(CategorizedPlaintextCorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids, cat_pattern, encoding='utf-8'):\n",
    "        super().__init__(root, fileids, cat_pattern=cat_pattern, encoding=encoding)\n",
    "        self.corpus_root = root\n",
    "\n",
    "    def resolve(self, fileids=None, categories=None):\n",
    "\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('Specify only one')\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        else:\n",
    "            return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as file:\n",
    "                yield file.read()\n",
    "        \n",
    "    def lines(self, fileids=None, categories=None):\n",
    "\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for line in doc.split('\\n'):\n",
    "                yield line\n",
    "\n",
    "    def line_parts(self, fileids=None, categories=None):\n",
    "\n",
    "        for line in self.lines(fileids, categories):\n",
    "            if line == '':\n",
    "                continue\n",
    "            yield line.split('#!#')\n",
    "\n",
    "    def intents(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, labels in self.line_parts(fileids, categories):\n",
    "            yield intent\n",
    "\n",
    "    def count_intents(self, fileids=None, categories=None):\n",
    "        \n",
    "        intents = set(intent for sentence, intent, labels in self.intents(fileids, categories))\n",
    "        return len(intents)\n",
    "\n",
    "    def slots(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, slots in self.line_parts(fileids, categories):\n",
    "            yield slots.split()\n",
    "\n",
    "    def count_slots(self, fileids=None, categories=None):\n",
    "        slots = set()\n",
    "        for entry in self.slots(fileids, categories):\n",
    "            for token in entry:\n",
    "                slots.add(token)\n",
    "        return len(slots)\n",
    "\n",
    "    def sentences(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, labels in self.line_parts(fileids, categories):\n",
    "            yield sentence\n",
    "\n",
    "    def sentences_and_labels(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, slots in self.line_parts(fileids, categories):\n",
    "            yield sentence, intent, slots\n",
    "\n",
    "    def all_slots(self):\n",
    "        \n",
    "        with open(self.corpus_root + '/slots.txt', 'r') as slots_file:\n",
    "            return slots_file.read().splitlines()\n",
    "\n",
    "    \n",
    "file_pattern = r'.*/[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\.txt'\n",
    "category_pattern = r'([^/]+)/[^/]+\\.txt$'\n",
    "corpus = IOBCorpusReader('data/corpus', file_pattern, cat_pattern=category_pattern)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe06743-f036-4612-8817-ef65f0714b1f",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 6: Bengfort, Benjamin, et al. Applied Text Analysis with Python : Enabling Language-Aware Data Products with Machine Learning, O'Reilly Media, Incorporated, 2018. ProQuest Ebook Central, https://ebookcentral.proquest.com/lib/fh-swf/detail.action?docID=5425029.\n",
    "* 7: NLTK Project, PlainTextCorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader.plaintext.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader, 17.03.2025]\n",
    "* 8: NLTK project, CorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader#nltk.corpus.reader.CorpusReader, 17.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24959113-27fe-480f-8dd0-19719694440b",
   "metadata": {},
   "source": [
    "## Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb037c-df46-409b-8d55-2a25fcf727aa",
   "metadata": {},
   "source": [
    "Beschreibung von BERT !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e052f41-aa2d-4b4a-9515-1b12387bccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertForTokenClassification\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4af26ee-e332-4a01-9e1b-16c5fc60d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten sammeln\n",
    "sentences = list(corpus.sentences())\n",
    "intents = list(corpus.intents())\n",
    "slots = list(corpus.slots())\n",
    "\n",
    "# Daten aufteilen\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_intents, test_intents, train_slots, test_slots = train_test_split(\n",
    "    sentences, intents, slots, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4402abf-6ff4-4365-99fb-3e53e92a492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69903d222d2f4d15b5b6c307c8803952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c295a352bfcb4d7e97e0c37da77c979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 02:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.055397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.202900</td>\n",
       "      <td>0.049934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=546, training_loss=0.18818364890067132, metrics={'train_runtime': 127.206, 'train_samples_per_second': 273.265, 'train_steps_per_second': 4.292, 'total_flos': 732427682531850.0, 'train_loss': 0.18818364890067132, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Labels kodieren\n",
    "intent_label_mapping = {intent: i for i, intent in enumerate(set(train_intents))}\n",
    "train_labels = torch.tensor([intent_label_mapping[intent] for intent in train_intents])\n",
    "test_labels = torch.tensor([intent_label_mapping[intent] for intent in test_intents])\n",
    "\n",
    "# Daten vorbereiten\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": test_labels\n",
    "})\n",
    "\n",
    "# Die Daten in das richtige Format bringen (falls nötig)\n",
    "def encode_examples(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(examples[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(examples[\"labels\"])\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(encode_examples, batched=True)\n",
    "test_dataset = test_dataset.map(encode_examples, batched=True)\n",
    "\n",
    "# Modell laden\n",
    "model_intent = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(intent_label_mapping))\n",
    "\n",
    "# Training konfigurieren\n",
    "training_args_intent = TrainingArguments(\n",
    "    output_dir=\"./intent_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=None,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer_intent = Trainer(\n",
    "    model=model_intent,\n",
    "    args=training_args_intent,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer_intent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1980a38e-a689-4f72-aa7a-4f9d69b25521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d8a1986fc9411b9a08cfc113358ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709976b2e245455eb430676ae52977aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.204244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.181804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=546, training_loss=0.3916260331541627, metrics={'train_runtime': 122.7048, 'train_samples_per_second': 283.29, 'train_steps_per_second': 4.45, 'total_flos': 727804840709808.0, 'train_loss': 0.3916260331541627, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization für Slot Filling\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings_slots = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings_slots = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Slots als Labels kodieren\n",
    "slot_label_mapping = {slot: i for i, slot in enumerate(corpus.all_slots())}\n",
    "train_slot_labels = [[slot_label_mapping[tag] for tag in tags] for tags in train_slots]\n",
    "test_slot_labels = [[slot_label_mapping[tag] for tag in tags] for tags in test_slots]\n",
    "\n",
    "# Labels auf die gleiche Länge wie die Eingaben aufpolstern\n",
    "def pad_labels(labels, max_len):\n",
    "    return [label + [0] * (max_len - len(label)) for label in labels]\n",
    "\n",
    "train_max_len = max(len(encoding) for encoding in train_encodings_slots['input_ids'])\n",
    "train_slot_labels = torch.tensor(pad_labels(train_slot_labels, train_max_len))\n",
    "\n",
    "test_max_len = max(len(encoding) for encoding in test_encodings_slots['input_ids'])\n",
    "test_slot_labels = torch.tensor(pad_labels(test_slot_labels, test_max_len))\n",
    "\n",
    "# Konvertieren zu Dataset\n",
    "train_dataset_slots = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings_slots[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings_slots[\"attention_mask\"],\n",
    "    \"labels\": train_slot_labels\n",
    "})\n",
    "\n",
    "test_dataset_slots = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings_slots[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings_slots[\"attention_mask\"],\n",
    "    \"labels\": test_slot_labels\n",
    "})\n",
    "\n",
    "# Sicherstellen, dass das Dataset die richtigen Typen hat\n",
    "def encode_examples_slots(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(examples[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(examples[\"labels\"])\n",
    "    }\n",
    "\n",
    "train_dataset_slots = train_dataset_slots.map(encode_examples_slots, batched=True)\n",
    "test_dataset_slots = test_dataset_slots.map(encode_examples_slots, batched=True)\n",
    "\n",
    "# Modell für Slot Filling\n",
    "model_slots = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(slot_label_mapping))\n",
    "\n",
    "# Training konfigurieren\n",
    "training_args_slots = TrainingArguments(\n",
    "    output_dir=\"./slot_filling_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_slots = Trainer(\n",
    "    model=model_slots,\n",
    "    args=training_args_slots,\n",
    "    train_dataset=train_dataset_slots, \n",
    "    eval_dataset=test_dataset_slots,\n",
    ")\n",
    "\n",
    "trainer_slots.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e3647-6347-4204-93e4-84a03fcbba93",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1902.10909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80078b75-1fb9-4126-8892-04b1cf7f86f5",
   "metadata": {},
   "source": [
    "## Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f665ef-9f05-49a1-93fa-9f6ed9c3551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
