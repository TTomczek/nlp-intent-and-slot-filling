{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e39565f-01fe-4d64-ad1a-83d69e597052",
   "metadata": {},
   "source": [
    "# Joint Intent detection and slot filling\n",
    "Dieses Jupyter notebook wurde als semesterabschließende Arbeit für das Modul Natural Language Processing an der [Fachhochschule Südwestfalen](https://www.fh-swf.de/en/international_3/index.php) erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e89dc9-708f-4c4f-8371-36f810242e7d",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "Das Joint intent detection and slot filling (IDSF) ist eine Aufgabe aus dem Teilbereich des Natural Language Understandings (NLU) des Natural Language Processings (NLP), die uns in heutzutage fast täglich Alltag begegnet. Sei es um einen Timer auf dem Handy zu starten, bestimmte Musik abzuspielen oder das Licht einzuschalten. Der Ablauf ist dabei häufig der selbe: \"Siri stelle einen Timer für 4 Minuten\", \"Alexa spiele meine Schlager Playlist\" oder \"Google erstelle einen Arzttermin für heute 16:00 Uhr\". Meist beginnen die Kommandos mit dem Namen des Sprachassistenten, um diesen zu aktivieren, gefolgt vom Kommando für die gewünschte Aktion. Das IDSF beschäftigt sich dabei mit der Aufgabe, die gewünschte Aktion (Intent), also stelle einen Timer, Spiele Musik, erstelle einen Termin im Kalender und die dazugehörigen notwendigen Parameter, wie z.B. vier Minuten, Schlager Playlist oder Arzt heute 16:00 Uhr (Slots) zu erkennen.\n",
    "Da der Gebrauch dieser Sprachassistenten in Zukunft wahrscheinlich noch stärker zu nehmen wird, wollen wir uns deren funktionsweise in diesem Notebook näher anschauen. Dafür wird zuerst die Entwicklungshistorie vom IDSF betrachtet und anschließend wird ein eigenes Modell für die Erkennung erstellt und anhand eines selbst vorbereiteten Korpus trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d088a2-b7f1-41f3-bd3b-1858acb800e7",
   "metadata": {},
   "source": [
    "## Joint Intent Detection and Slot filling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bb627-85be-4f27-b800-9860a15cb12f",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f3c76-2bf2-49ff-9c87-6a1841c2c9ef",
   "metadata": {},
   "source": [
    "## Datenbeschaffung\n",
    "\n",
    "Als Datensatz für das nachfolgende Beispiel verwenden wir den Snips-Datensatz. Dieser Datensatz wurde vom, mittlerweil zu Sonos gehörenden [1], [Snips Team](https://snips.ai/) zusammengestellt, um ihr eigenes Modell mit anderen Wettbewerbern wie zum Beispiel Amazons Alexa zu verglichen. Die Ergebnisse und die Datensätze der drei Vergleiche wurden in einem [GitHub Repository](https://github.com/sonos/nlu-benchmark/tree/master) veröffentlicht und in dem Paper \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces\" [2] erläutert. Das Repository enthält die Daten für drei Evaluationen aus den Jahren 2016 bis 2018. Wir werden in diesem Notebook die Daten der 2017 durchgeführten Evaluation verwenden, da diese Sätze für sieben unterschiedliche und allgemeine Aufgaben enthält.\n",
    "\n",
    "Die Daten sind im dem Repository in einzelnen JSON-Dateien enthalten. Dabei gibt es pro Aufgabe zwei Dateien, eine für das Training und eine für die Validierung. Der Einfachheit halber wurden die Dateien in dem data Verzeichnis, dass diesem Notebook beiliegt, abgelegt.\n",
    "\n",
    "Nachfolgend ist ein Auszug aus der `train_AddToPlaylist_full.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04077bd-4c1c-475b-bd64-997f7afd2ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"AddToPlaylist\": [\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"Add another \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"song\",\n",
      "          \"entity\": \"music_item\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" to the \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Cita Romántica\",\n",
      "          \"entity\": \"playlist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"add \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"clem burke\",\n",
      "          \"entity\": \"artist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" in \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"my\",\n",
      "          \"entity\": \"playlist_owner\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Pre-Party R&B Jams\",\n",
      "          \"entity\": \"playlist\"\n",
      "        }\n",
      "      ]\n",
      "    },\n"
     ]
    }
   ],
   "source": [
    "!head -n 48 data/train_AddToPlaylist_full.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7977d9c-5cc6-4725-9386-46716911c4f5",
   "metadata": {},
   "source": [
    "Die Datei besteht an oberster Stelle aus dem Namen der gewünschten Aktion gefolgt von einer Liste an Objekten mit einem `Data` Attribut. Dieses enthält wiederum eine Liste von Objekten mit `Text` Attributen die den Satz in Teilen enthält. Dabei wird der Satz durch den Text eines definierten `Entities` geteilt. So enthält das erste Beispiel den Text bis zum ersten `entity` das als `music_item` klassifiziert wurde und wieder den gesamten Text bis zum nächsten entity, dem Namen einer Playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5966d-61dc-45af-8107-2fbf9f55b2de",
   "metadata": {},
   "source": [
    "Als nächstes werden die Daten in ein verwendbares Format transformiert. Ein in der Natural language processing gängiges Format ist das IOB Format. IOB steht für Inside-Outside-Beginning. Dieses Format ermöglicht die Kennzeichnung der einzelnen Entitäten in einem Satz. Es wird unteranderem von den weit verbreiteten Python Bibliotheken `NLTK` und `spaCy` unterstützt [3, 4]. Das Format wurde 1995 von Lance A. Ramshaw und Mitchell P. Marcus erfunden.\n",
    "\n",
    "Dieses Beispiel zeigt das Format einer Zeile, welches nachfolgend aus den JSON-Dateien erzeugt wird.\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o i-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "Am Beginn der Zeile steht der vollständige Satz abgetrennt durch ein BOS (begin of sentence) am Anfang des Satzes und ein EOS (end of sentence) am Ende des Satzes. Nun folgt das eigentliche IOB-Format. Dabei wird für jeden Token entweder der Buchstabe 'o', dieser steht für keine Bedeutung, der Buchstabe 'b', für den Beginn einer Entität die aus mehreren Token besteht, oder 'i', als Entität. Das 'i' steht dabei entweder nach einem 'b' wodurch eine Entität gekennzeichent wird, die aus mehreren Token besteht oder alleine für eine Entität die aus nur einem Token besteht.  Das 'b' und 'i' werden dabei jeweils gefolgt vom einem trennenden Bindestrich und der Entitätskategorie verwender. So ist der Name 'Clem Burke' unterteilt in ein `b-music_item` für Clem und `i-Music_item` für Burke. Dadurch wird definiert, dass die beiden Teile zusammen gehören.\n",
    "\n",
    "Das IOB2 Format ist eine Erweiterung des originalen IOB Formats. Es definiert das auch eine Entität die nur aus einem Token besteht mit einem 'b' kodiert wird und nicht wie im IOB Format mit einem 'i'. Dadurch ergibt sich das folgende Format:\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o b-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "\n",
    "Mit dem folgenden Python Code wird der Inhalt der im data-Verzeichnis liegenden Dateien in das vorgestellte Format transformiert. Die Dateien werden dabei im Verzeichnis `data/corpus` und einem Verzeichnis mit dem Titel der Intent-Kategorie abgelegt. Als Dateinamen werden `UUID`s verwendet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23b37ed-4ed4-4f10-b47f-d9856cac14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from shutil import rmtree\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049daa05-aa7a-4076-b36a-f4deb1ee1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alte Corpus-Dateien gelöscht\n",
      "Das Pfad data existiert nicht oder ist kein Verzeichnis.\n",
      "Try converting file at path data/validate_PlayMusic.json\n",
      "Finished converting file at path data/validate_PlayMusic.json. Writing to file...\n",
      "Try converting file at path data/train_SearchCreativeWork_full.json\n",
      "Finished converting file at path data/train_SearchCreativeWork_full.json. Writing to file...\n",
      "Try converting file at path data/train_AddToPlaylist_full.json\n",
      "Finished converting file at path data/train_AddToPlaylist_full.json. Writing to file...\n",
      "Try converting file at path data/train_RateBook_full.json\n",
      "Finished converting file at path data/train_RateBook_full.json. Writing to file...\n",
      "Try converting file at path data/train_SearchScreeningEvent_full.json\n",
      "Finished converting file at path data/train_SearchScreeningEvent_full.json. Writing to file...\n",
      "Try converting file at path data/validate_GetWeather.json\n",
      "Finished converting file at path data/validate_GetWeather.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchCreativeWork.json\n",
      "Finished converting file at path data/validate_SearchCreativeWork.json. Writing to file...\n",
      "Try converting file at path data/train_GetWeather_full.json\n",
      "Finished converting file at path data/train_GetWeather_full.json. Writing to file...\n",
      "Try converting file at path data/train_PlayMusic_full.json\n",
      "Finished converting file at path data/train_PlayMusic_full.json. Writing to file...\n",
      "Try converting file at path data/validate_RateBook.json\n",
      "Finished converting file at path data/validate_RateBook.json. Writing to file...\n",
      "Try converting file at path data/validate_AddToPlaylist.json\n",
      "Finished converting file at path data/validate_AddToPlaylist.json. Writing to file...\n",
      "Try converting file at path data/validate_BookRestaurant.json\n",
      "Finished converting file at path data/validate_BookRestaurant.json. Writing to file...\n",
      "Try converting file at path data/train_BookRestaurant_full.json\n",
      "Finished converting file at path data/train_BookRestaurant_full.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchScreeningEvent.json\n",
      "Finished converting file at path data/validate_SearchScreeningEvent.json. Writing to file...\n",
      "Finished converting all files!\n"
     ]
    }
   ],
   "source": [
    "def clean_formatted(formatted_directory_path):\n",
    "    if not os.path.exists(formatted_directory_path) or not os.path.isdir(formatted_directory_path):\n",
    "        print(f'Pfad {formatted_directory_path} ist kein Verzeichnis oder existiert nicht')\n",
    "        return\n",
    "\n",
    "    rmtree(formatted_directory_path)\n",
    "    print('Alte Corpus-Dateien gelöscht')\n",
    "\n",
    "def convert_file(json_file_path, corpus_root):\n",
    "    print(f'Try converting file at path {json_file_path}')\n",
    "    if not os.path.isfile(json_file_path):\n",
    "        print(f'File {file_path} does not exists', json_file_path)\n",
    "        return \n",
    "    formatted_lines = []\n",
    "    intent_category = None\n",
    "    all_slots_set = set()\n",
    "    all_slots_set.add('o')\n",
    "    with open(json_file_path, 'r', encoding='latin-1') as json_file:\n",
    "        json_content = json.load(json_file)\n",
    "        intent_category = next(iter(json_content))\n",
    "        for sentence_block in json_content[intent_category]:\n",
    "            sentence = \"\"\n",
    "            slots = []\n",
    "            for sentence_data_block in sentence_block['data']:\n",
    "                sentence_part = sentence_data_block['text']\n",
    "                sentence_part = re.sub('\\n', '', sentence_part)\n",
    "                if sentence_part != '':\n",
    "                    sentence += sentence_part\n",
    "                sentence_part_len = len(sentence_part.split())\n",
    "                if 'entity' in sentence_data_block:\n",
    "                    entity_type = sentence_data_block['entity']\n",
    "                    if sentence_part_len > 1:\n",
    "                        firstSlot = True\n",
    "                        for i in range(sentence_part_len):\n",
    "                            if firstSlot:\n",
    "                                slots.append('b-' + entity_type)\n",
    "                                firstSlot = False\n",
    "                                all_slots_set.add('b-' + entity_type)\n",
    "                            else:\n",
    "                                slots.append('i-' + entity_type)\n",
    "                                all_slots_set.add('i-' + entity_type)\n",
    "                    else:\n",
    "                        slots.append('b-' + entity_type)\n",
    "                        all_slots_set.add('b-' + entity_type)\n",
    "                else:\n",
    "                    for i in range(sentence_part_len):\n",
    "                        slots.append('o')\n",
    "            formatted_lines.append(construct_row(sentence, intent_category, slots))\n",
    "    print(f'Finished converting file at path {json_file_path}. Writing to file...')\n",
    "    write_to_file(intent_category, formatted_lines, corpus_root)\n",
    "    return all_slots_set\n",
    "    \n",
    "\n",
    "def construct_row(sentence, intent, slots):\n",
    "    row = ''\n",
    "    row += sentence\n",
    "    row += '#!#'\n",
    "    row += intent\n",
    "    row += '#!#'\n",
    "    row += ' '.join(slots)\n",
    "    row += '\\n'\n",
    "    return row\n",
    "\n",
    "\n",
    "def write_to_file(intent, lines, corpus_root):\n",
    "    if intent is None or intent == '':\n",
    "        print('No intent')\n",
    "        return\n",
    "        \n",
    "    base_output_directory = corpus_root\n",
    "    output_file_path = base_output_directory + intent + '/' + str(uuid.uuid4()) + '.txt'\n",
    "\n",
    "    if not os.path.exists(base_output_directory + intent): \n",
    "        os.makedirs(base_output_directory + intent)\n",
    "    \n",
    "    with open(output_file_path, 'a') as output_file:\n",
    "        output_file.writelines(lines)\n",
    "\n",
    "\n",
    "def write_slots_to_file(corpus_root, slots):\n",
    "    with open(corpus_root + 'slots.txt', 'w') as slots_file:\n",
    "        lines = [slot + \"\\n\" for slot in slots]\n",
    "        slots_file.writelines(lines)\n",
    "\n",
    "\n",
    "def iterate_over_json_files_in_directory(directory_path, corpus_root):\n",
    "    if not os.path.exists(directory_path) or directory_path is not os.path.isdir(directory_path):\n",
    "        print(f\"Das Pfad {directory_path} existiert nicht oder ist kein Verzeichnis.\")\n",
    "\n",
    "    slots = set()\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):  # Nur JSON-Dateien berücksichtigen\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            slots_from_file = convert_file(file_path, corpus_root)\n",
    "        slots.update(slots_from_file)\n",
    "\n",
    "    write_slots_to_file(corpus_root, slots)\n",
    "    print('Finished converting all files!')\n",
    "    \n",
    "    \n",
    "clean_formatted('data/corpus')\n",
    "iterate_over_json_files_in_directory('data', 'data/corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dd92-a1b1-404d-b1af-e13fa5ebbfa4",
   "metadata": {},
   "source": [
    "Als nächstes wird geprüft, ob auch alle Einträge in der Textdatei enthalten sind. Dafür werden die Einträge in der train- und validate.json mit der Anzahl der Zeilen in der Textdatei verglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99108436-e37e-4954-be8d-66a2e8104be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: data/formatted/RateBook.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579d5d3c-fc2a-490c-98fe-c98bf612674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m1956\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/train_RateBook_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431915fc-2450-4617-aad4-37ecd6e94991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/validate_RateBook.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490f728-7789-4273-902a-8f9f11235122",
   "metadata": {},
   "source": [
    "Man sieht, dass die Anzahl der `data`-Blöcke aus den JSON-Dateien der Anzahl der Zeilen in der erzeugten Textdatei entspricht. Die Konvertierung war also erfolgreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32f47ef-983c-4ba5-a359-a808e2cf2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS rate The Lotus and the Storm zero of 6 EOS RateBook o b-object_name i-object_name i-object_name i-object_name i-object_name b-rating_value o b-best_rating\n",
      "BOS Rate The Fall-Down Artist 5 stars. EOS RateBook o b-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS Rate the current novel one points EOS RateBook o o b-object_select b-object_type b-rating_value b-rating_unit\n",
      "BOS rate The Ape-Man Within 4 EOS RateBook o b-object_name i-object_name i-object_name b-rating_value\n",
      "BOS I give The Penalty three stars EOS RateBook o o b-object_name i-object_name b-rating_value b-rating_unit\n",
      "BOS rate this novel a 4 EOS RateBook o b-object_select b-object_type o b-rating_value\n",
      "BOS give 5 out of 6 points to Absolutely, Positively Not series EOS RateBook o b-rating_value o o b-best_rating b-rating_unit o b-object_name i-object_name i-object_name b-object_part_of_series_type\n",
      "BOS I give Emile, or On Education five points. EOS RateBook o o b-object_name i-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS rate Licence Renewed a 4 EOS RateBook o b-object_name i-object_name o b-rating_value\n",
      "BOS Give this essay a 2 out of 6. EOS RateBook o b-object_select b-object_type o b-rating_value o o b-best_rating o\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9ff63-9acc-462e-b20b-c39fcb78caf4",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 1: https://investors.sonos.com/news-and-events/investor-news/latest-news/2019/Sonos-Announces-Acquisition-of-Snips/default.aspx, [Online, 07.03.2025]\n",
    "* 2: Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.\" 2018, [Online: https://arxiv.org/abs/1805.10190, 07.03.2025]\n",
    "* 3: NLTK Team, tree2conlltags, [Online: https://www.nltk.org/_modules/nltk/chunk/util.html#tree2conlltags, 13.03.2025]\n",
    "* 4: Explosion, spaCy convert, [Online: https://spacy.io/api/cli#converters, 13.03.2025]\n",
    "* 5: Ramshaw und Marcus, \"Text Chunking using Transformation-Based Learning\" 1995, [Online: https://arxiv.org/abs/cmp-lg/9505040, 14.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511baabd-4337-4292-9483-482fd44392e8",
   "metadata": {},
   "source": [
    "## Erstellen eines Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc2109-e2af-4849-a245-6867a42bea03",
   "metadata": {},
   "source": [
    "Nachdem nun die Daten in ein verwendbares Format tranformiert wurden, müssen wir den effiziente Zugriff auf die Daten sicherstellen. Dafür bietet die Python-Bibliothek NLTK verschiedene `CorpusReader` Klassen zur Verfügung. Diese Klassen ermöglichen über Methoden den Zugriff auf die Dokumente selbst, sowie auf weitere Dateien eines Corpus, wie zum Beispiel die Lizenz des Corpus oder eine README.md Datei. NLTK bietet eine große Anzahl an CorpusReadern für verschiedenste Szenarien an. Ein einfacher CorpusReader ist der `PlainTextCorpusReader` [7]. Dieser bietet Zugriff auf reine Textdateien. Eine vollständige Liste ist in der [Dokumentation](https://www.nltk.org/api/nltk.corpus.reader.html) von NLTK zu finden.\n",
    "\n",
    "Für das zuvor definierte Format gibt es keinen passenden vorgefertigten Reader. Daher müssen wir einen eigenen erstellen. Dafür kann die `CorpusReader`-Basisklasse [8] erweitert werden oder ein bestehender verwendet werden. Wir werden den `CategorizedPlaintextCorpusReader` als Basis verwenden. Dieser ist eine Erweiterung des zuvor erwähnten `PlainTextCorpusReader`, welcher Zugriff auf Textdateien bietet.  Durch die Nutzung des `CategorizedPlaintextCorpusReader` können die Dateien zusätzlich in Kategorien unterteilt werden. Dies geschieht über reguläre Ausdrücke. Im nachfolgenden Code definiert der reguläre Ausdruck _category_pattern_ die Kategorie als den Namen des Verzeichnisses, indem die Textdateien liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c56201-542f-4d46-8644-27e5da9e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from nltk import wordpunct_tokenize\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe06789a-d306-4896-a9f9-0640f8d0d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOBCorpusReader(CategorizedPlaintextCorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids, cat_pattern, encoding='utf-8'):\n",
    "        super().__init__(root, fileids, cat_pattern=cat_pattern, encoding=encoding)\n",
    "        self.corpus_root = root\n",
    "\n",
    "    def resolve(self, fileids=None, categories=None):\n",
    "\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('Specify only one')\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        else:\n",
    "            return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as file:\n",
    "                yield file.read()\n",
    "        \n",
    "    def lines(self, fileids=None, categories=None):\n",
    "\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for line in doc.split('\\n'):\n",
    "                yield line\n",
    "\n",
    "    def line_parts(self, fileids=None, categories=None):\n",
    "\n",
    "        for line in self.lines(fileids, categories):\n",
    "            if line == '':\n",
    "                continue\n",
    "            yield line.split('#!#')\n",
    "\n",
    "    def intents(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, labels in self.line_parts(fileids, categories):\n",
    "            yield intent\n",
    "\n",
    "    def count_intents(self, fileids=None, categories=None):\n",
    "        \n",
    "        intents = set(intent for sentence, intent, labels in self.intents(fileids, categories))\n",
    "        return len(intents)\n",
    "\n",
    "    def slots(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, slots in self.line_parts(fileids, categories):\n",
    "            yield slots.split()\n",
    "\n",
    "    def count_slots(self, fileids=None, categories=None):\n",
    "        slots = set()\n",
    "        for entry in self.slots(fileids, categories):\n",
    "            for token in entry:\n",
    "                slots.add(token)\n",
    "        return len(slots)\n",
    "\n",
    "    def sentences(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, labels in self.line_parts(fileids, categories):\n",
    "            yield sentence\n",
    "\n",
    "    def sentences_and_labels(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, slots in self.line_parts(fileids, categories):\n",
    "            yield sentence, intent, slots\n",
    "\n",
    "    def all_slots(self):\n",
    "        \n",
    "        with open(self.corpus_root + '/slots.txt', 'r') as slots_file:\n",
    "            return slots_file.read().splitlines()\n",
    "\n",
    "    \n",
    "file_pattern = r'.*/[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\.txt'\n",
    "category_pattern = r'([^/]+)/[^/]+\\.txt$'\n",
    "corpus = IOBCorpusReader('data/corpus', file_pattern, cat_pattern=category_pattern)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe06743-f036-4612-8817-ef65f0714b1f",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 6: Bengfort, Benjamin, et al. Applied Text Analysis with Python : Enabling Language-Aware Data Products with Machine Learning, O'Reilly Media, Incorporated, 2018. ProQuest Ebook Central, https://ebookcentral.proquest.com/lib/fh-swf/detail.action?docID=5425029.\n",
    "* 7: NLTK Project, PlainTextCorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader.plaintext.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader, 17.03.2025]\n",
    "* 8: NLTK project, CorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader#nltk.corpus.reader.CorpusReader, 17.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24959113-27fe-480f-8dd0-19719694440b",
   "metadata": {},
   "source": [
    "## Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb037c-df46-409b-8d55-2a25fcf727aa",
   "metadata": {},
   "source": [
    "Nachdem nun die Daten vorbereitet wurden und der Zugriff über einen CorpusReader komfortabel möglich ist. Widmen wir uns den Modellen die wir für die Analyse der Infos nutzen wollen.\n",
    "Zuerst werden zwei getrennte Modelle verwendet. Eines für die Intent Klassifikation und eines für die Bestimmung der Slots. Anschließend wird der neuere Ansatz der gemeinsamen Bestimmung des Intents und der Slots gezeigt. Dieses Ansatz bringt den Vorteil, dass der Text nur einmal verareitet werden muss. Zusätzlich bringt es den Vorteil, dass die Zusammenhänge von Intents und Slots berücksichtigt werden können. So kann der Intent oder die Slots jeweils helfen, das andere Ergebnis zu verbessern. Ein Beispiel wäre die Buchung von einem Flug oder einem Tisch in einem Restaurant. So kann ein Satz zur Buchung mit zwei enthaltenen Ortsangaben besser kategorisiert werden, wenn etwas über den Kontext bekannt ist. Sind zwei Städte in dem Befehl angegeben handelt es sich wahrscheinlich eher um die Buchung eines Fluges anstatt der Buchung in einem Restaurant. Wohingegen die Buchung eines Restaurant eher auf nur eine Stadt hindeutet und die zweite Ortsangabe eventuell aufschluss über einen Stadtteil oder die gewüschte Lage des Tisches gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e052f41-aa2d-4b4a-9515-1b12387bccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertForTokenClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e5ce8-5ccc-4445-bb51-77cf4f78a99a",
   "metadata": {},
   "source": [
    "Zuerst holen wir die Daten für das Training aus dem Corpus. Dafür werden die Methoden für die einzelnen Bestandteile verwendet. Anschließend verwenden wir die Funktion `train_test_split` aus dem SciKit-Learn Paket um diese in Trainings- und Validierungsteile zu trennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4af26ee-e332-4a01-9e1b-16c5fc60d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(corpus.sentences())\n",
    "intents = list(corpus.intents())\n",
    "slots = list(corpus.slots())\n",
    "\n",
    "train_sentences, test_sentences, train_intents, test_intents, train_slots, test_slots = train_test_split(\n",
    "    sentences, intents, slots, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11300492-e2b4-4189-ab09-fb0950ba9a22",
   "metadata": {},
   "source": [
    "In den nachfolgenden Beispielen werden Varianten des BERT Modells verwenden. BERT ist ein bidirektionaler encoder-only Transformer. Es wurde im Oktober 2018 von Forschern bei Google vorgestellt. BERT wurde mittels 'masked token prediction', also das Vorhersagen eines maskierten Tokens basierend auf den vorherigen Token, auf dem BookCorpus und dem Englischen Wikipedia trainiert. Durch seine Architektur kann ein bereits trainiertes BERT Modell durch das hinzufügen einer einzigen neuen Schicht für verschiedenste Aufgaben angepasst werden [9].\n",
    "In den folgenden Beispielen werden die von HuggingFace bereitgestellten Varianten verwendet. Die Transformers Bibliothek von HuggingFace bietet unter anderem folgende Varianten von Bert an: `BertForNextSentencePrediction`, `BertForQuestionAnswering` und, für unser Beispiel relevant, `BertForSequenceClassification` für die Klassifikation von Sätzen und `BertForTokenClassification` für die Klassifizierung von einzelnen Token, in diesem Fall Wörtern, an [10].\n",
    "BERT ist außerdem in verschiedenen Ausführungen verfügbar die sich in der Anzahl der Parameter und den verwendeten Trainigsdaten unterscheiden. Im weiteren Verlauf wird als Basis das `bert-base-uncased` Modell verwendet. Dieses besitzt 110M Parameter und wurde auf einem 'uncased' Datensatz, nur Kleinbuchstaben, trainiert. Als weitere Ausführungen gibt es noch die 'large' Variante mit 340M Parametern und jeweils eine 'cased' Variante mit Groß- und Kleinschreibung [11]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a0b32-7d72-42f6-82f6-86f8e0f3cd1a",
   "metadata": {},
   "source": [
    "Quellen:\n",
    "\n",
    "* 9: Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018, [Online: https://arxiv.org/abs/1810.04805, 23.06.2025]\n",
    "* 10: HuggingFace Inc., Transformers BERT, 2025, [Online: https://arxiv.org/abs/1810.04805, 23.06.2025]\n",
    "* 11: HuggingFace Inc., BERT release, 2025, [Online: https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc, 23.06.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccf3ca-8734-4ff6-8175-a0909d51c623",
   "metadata": {},
   "source": [
    "Da in den nachfolgenden Beispielen immer ein Tokenizer benötigt wird und dieser nicht Modellunanhängig sein muss, wird er hier zu Anfang ein defineirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be27f4f6-98d4-440c-b808-982798f48158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad73a39-0461-42ce-9539-8a39bb208fcc",
   "metadata": {},
   "source": [
    "In der folgenden Zelle trainieren wir das `BertForSequenceClassification` Modell für die Klassifikation der Intents aus unseren Daten.\n",
    "Zuerst wird der `BertTokenizer` verwendet um aus den Sätzen die Encodings zu erstellen. Durch die Parameter padding werden die Sätze auf eine einheitliche Länge gebracht, indem Padding-Token angehangen werden. Sätze die Länger als die vom Modell unterstützte Länge sind werden mit dem Parameter `truncation` gekürzt.\n",
    "Im zweiten Abschnitt werden die Text-Intents in Nummernlabel umgewandelt.\n",
    "Anschließend werden die Encodings in `Datasets` umgewandelt dabei werden die erzeugten \"input_ids\" und die \"attention_mask\", die die Padding-Token vom eigentlichen Satz unterscheidet, verwendet.\n",
    "Die vorbereiteten Daten werden dann mit einem HuggingFace `Trainer` für das Training an des BertForSequenceClassification Modell übergeben und das Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4402abf-6ff4-4365-99fb-3e53e92a492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a38cb8c376460da6c4fa315b85a14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef896f90a2a4786ab25a35a02a14c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='546' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [546/546 02:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.063290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.056072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.052450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=546, training_loss=0.19020338089038163, metrics={'train_runtime': 130.7018, 'train_samples_per_second': 265.957, 'train_steps_per_second': 4.177, 'total_flos': 732427682531850.0, 'train_loss': 0.19020338089038163, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "intent_label_mapping = {intent: i for i, intent in enumerate(set(train_intents))}\n",
    "train_labels = torch.tensor([intent_label_mapping[intent] for intent in train_intents])\n",
    "test_labels = torch.tensor([intent_label_mapping[intent] for intent in test_intents])\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"labels\": train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "    \"labels\": test_labels\n",
    "})\n",
    "\n",
    "def encode_examples(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(examples[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(examples[\"labels\"])\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(encode_examples, batched=True)\n",
    "test_dataset = test_dataset.map(encode_examples, batched=True)\n",
    "\n",
    "model_intent = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(intent_label_mapping))\n",
    "\n",
    "training_args_intent = TrainingArguments(\n",
    "    output_dir=\"./intent_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=None,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer_intent = Trainer(\n",
    "    model=model_intent,\n",
    "    args=training_args_intent,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer_intent.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b37c59-9957-4e56-8b46-ece1f1a32525",
   "metadata": {},
   "source": [
    "Da jetzt das Modell für die Klassifikation der Sätze zu den Intents fertig ist, behandeln wir im nächsten Schritt das Modell für die Klassifikation der Token.\n",
    "Da der BertTokenizer Wörter in Subwords unterteilt kann es passieren, dass die IOB-Label nicht mehr zu passen. Um dies zu beheben sorgt die nachfolgende FUnktion dafür, dass bei einem Subword, welche immer mit einem `##` gekenzeichnet sind, ein entsprechenes IOB-Label eingefügt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f569a22-3552-4b38-830b-2d034c32e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_iob_and_collect_set(encoded_sentences, labels, tokenizer):\n",
    "    aligned_labels = []\n",
    "    new_labels_set = set()\n",
    "    \n",
    "    for input_ids, sentence_labels in zip(encoded_sentences[\"input_ids\"], labels):\n",
    "        expanded_labels = []\n",
    "        label_index = 0\n",
    "        \n",
    "        for token_id in input_ids:\n",
    "            token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
    "            \n",
    "            if token.startswith(\"##\"):\n",
    "                # Subword bekommt ein I-Label (falls vorhanden)\n",
    "                if sentence_labels[label_index].startswith(\"B-\"):\n",
    "                    new_label = \"I-\" + sentence_labels[label_index][2:]\n",
    "                    expanded_labels.append(new_label)\n",
    "                else:\n",
    "                    new_label = sentence_labels[label_index]\n",
    "                    expanded_labels.append(new_label)\n",
    "            else:\n",
    "                # Hauptwörter bekommen das ursprüngliche Label\n",
    "                new_label = sentence_labels[label_index]\n",
    "                expanded_labels.append(new_label)\n",
    "                label_index += 1\n",
    "            \n",
    "            # Füge das neue Label zum Set hinzu\n",
    "            new_labels_set.add(new_label)\n",
    "\n",
    "            # Sicherheitscheck, um Index-Überläufe zu vermeiden\n",
    "            if label_index >= len(sentence_labels):\n",
    "                break\n",
    "\n",
    "        aligned_labels.append(expanded_labels)\n",
    "    \n",
    "    return aligned_labels, new_labels_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9d85f-31c5-4999-a749-8c97953d8b07",
   "metadata": {},
   "source": [
    "Bei diesem Modell ist das Vorgehen, wie bei dem Vorherigen. Die Label werden mit dem Tokeinzer verarbeitet und danach mit der zuvor definierten Funktion aufbereitet. Die Aufbereitung gibt als Ergebnis zunächst die Label zurück und eine Liste an neu erstellten Label. Dies ist notwendig da durch die Aufteilung in Subwords, zuvor nicht vorkommende I-Label entstanden sein können.\n",
    "Jetzt wird mit dem Labeln des Corpus und den neuen Labeln wieder ein Mapping auf Zahlen durchgeführt.\n",
    "Die codierten Label werden dann auf eine gemeinsame Länge gepadded und anschließend wieder in ein `Dataset` überführt und mittels des Trainers für das Training des Modells verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1980a38e-a689-4f72-aa7a-4f9d69b25521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd104022918a492d99791551090f78fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8b8aed69404bc5b55a0f9eb036cf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2897 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1820' max='1820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1820/1820 06:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.171944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.121745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.097350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.090857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.076714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.071514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.069398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.065737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.065329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1820, training_loss=0.1564936297280448, metrics={'train_runtime': 399.1398, 'train_samples_per_second': 290.299, 'train_steps_per_second': 4.56, 'total_flos': 2426016135699360.0, 'train_loss': 0.1564936297280448, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings_slots = tokenizer(train_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_encodings_slots = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_adjusted_labels, train_new_label = align_labels_with_iob_and_collect_set(train_encodings_slots, train_slots, tokenizer)\n",
    "test_adjusted_labels, test_new_label = align_labels_with_iob_and_collect_set(test_encodings_slots, test_slots, tokenizer)\n",
    "\n",
    "all_label_corpus = set(corpus.all_slots())\n",
    "all_label_corpus.update(train_new_label)\n",
    "all_label_corpus.update(test_new_label)\n",
    "\n",
    "slot_label_mapping = {slot: i for i, slot in enumerate(all_label_corpus)}\n",
    "train_slot_labels = [[slot_label_mapping[tag] for tag in tags] for tags in train_slots]\n",
    "test_slot_labels = [[slot_label_mapping[tag] for tag in tags] for tags in test_slots]\n",
    "\n",
    "# Hier Fehler weil 0 = I_country??!?!\n",
    "def pad_labels(labels, max_len):\n",
    "    return [label + [0] * (max_len - len(label)) for label in labels]\n",
    "\n",
    "train_max_len = max(len(encoding) for encoding in train_encodings_slots['input_ids'])\n",
    "train_slot_labels = torch.tensor(pad_labels(train_slot_labels, train_max_len))\n",
    "\n",
    "test_max_len = max(len(encoding) for encoding in test_encodings_slots['input_ids'])\n",
    "test_slot_labels = torch.tensor(pad_labels(test_slot_labels, test_max_len))\n",
    "\n",
    "train_dataset_slots = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings_slots[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings_slots[\"attention_mask\"],\n",
    "    \"labels\": train_slot_labels\n",
    "})\n",
    "\n",
    "test_dataset_slots = Dataset.from_dict({\n",
    "    \"input_ids\": test_encodings_slots[\"input_ids\"],\n",
    "    \"attention_mask\": test_encodings_slots[\"attention_mask\"],\n",
    "    \"labels\": test_slot_labels\n",
    "})\n",
    "\n",
    "def encode_examples_slots(examples):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(examples[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(examples[\"attention_mask\"]),\n",
    "        \"labels\": torch.tensor(examples[\"labels\"])\n",
    "    }\n",
    "\n",
    "train_dataset_slots = train_dataset_slots.map(encode_examples_slots, batched=True)\n",
    "test_dataset_slots = test_dataset_slots.map(encode_examples_slots, batched=True)\n",
    "\n",
    "model_slots = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(slot_label_mapping))\n",
    "\n",
    "training_args_slots = TrainingArguments(\n",
    "    output_dir=\"./slot_filling_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer_slots = Trainer(\n",
    "    model=model_slots,\n",
    "    args=training_args_slots,\n",
    "    train_dataset=train_dataset_slots, \n",
    "    eval_dataset=test_dataset_slots,\n",
    ")\n",
    "\n",
    "trainer_slots.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadce61d-efab-456d-9aa9-11bb89bd2f03",
   "metadata": {},
   "source": [
    "In der nachfolgenden Zelle wird eine Funktion definiert, mit der wir auf einen Satz eine getrennte Vorhersage mit den zwei Modellen durchführen können. Danach werden mehrere Sätze angegeben, um die die Modelle zu testen.\n",
    "Die Funktion erwartet einen Satz, ein Modell für die Intent Detection, eines für das Slot Filling, einen Tokenizer und das Gerät auf dem die Vorhersagen ausgeführt werden sollen.\n",
    "In der Funktion wird zunächst der Satz codiert und auf das gewählte Gerät verschoben. Danach wird der Satz an beide Modelle übergeben und die Vorhersagen ausgegeben. Die Modelle werden für die Verwendung in den Evaluierungsmodus versetzt und auf das gewählte Gerät, in der Regel die GPU, verschoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8deaf0b4-e965-481b-bfce-25593518174e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence to inspect: Add Jimmy Hendrix to my metal playlist\n",
      "Predicted Intent: AddToPlaylist\n",
      "Predicted Slots: ['o', 'b-artist', 'i-artist', 'o', 'b-playlist_owner', 'b-playlist', 'o', 'i-country', 'i-country', 'i-country']\n",
      "\n",
      "Sentence to inspect: Book a table in the finest restaurant in Paris\n",
      "Predicted Intent: BookRestaurant\n",
      "Predicted Slots: ['o', 'o', 'o', 'o', 'o', 'b-sort', 'b-restaurant_type', 'o', 'b-state', 'i-country', 'i-country']\n",
      "\n",
      "Sentence to inspect: I want to book a flight to New York tomorrow morning.\n",
      "Predicted Intent: BookRestaurant\n",
      "Predicted Slots: ['o', 'o', 'o', 'o', 'o', 'o', 'o', 'b-state', 'i-city', 'o', 'i-timeRange', 'o', 'i-country', 'i-country']\n",
      "\n",
      "Sentence to inspect: What's the weather in DC?\n",
      "Predicted Intent: GetWeather\n",
      "Predicted Slots: ['o', 'o', 'o', 'o', 'b-state', 'o', 'i-country', 'i-country', 'i-country', 'i-country']\n",
      "\n",
      "Sentence to inspect: How is the weather in DC?\n",
      "Predicted Intent: GetWeather\n",
      "Predicted Slots: ['o', 'o', 'o', 'o', 'o', 'b-state', 'o', 'i-country', 'i-country']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Funktion für Intent Detection und Slot Filling\n",
    "def predict_intent_and_slots(sentence, model_intent, model_slots, tokenizer, device):\n",
    "    # Eingabe vorbereiten\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Intent-Erkennung\n",
    "    model_intent.eval()\n",
    "    model_intent.to(device)\n",
    "    with torch.no_grad():\n",
    "        intent_outputs = model_intent(**inputs)\n",
    "    intent_prediction = torch.argmax(intent_outputs.logits, dim=1).item()\n",
    "    intent_label = list(intent_label_mapping.keys())[intent_prediction]\n",
    "\n",
    "    # Slot-Filling\n",
    "    model_slots.eval()\n",
    "    model_slots.to(device)\n",
    "    with torch.no_grad():\n",
    "        slot_outputs = model_slots(**inputs)\n",
    "    slot_predictions = torch.argmax(slot_outputs.logits, dim=-1).squeeze().tolist()\n",
    "    slot_tags = [list(slot_label_mapping.keys())[tag] for tag in slot_predictions]\n",
    "\n",
    "    print(f\"Sentence to inspect: {sentence}\")\n",
    "    print(f\"Predicted Intent: {intent_label}\")\n",
    "    print(f\"Predicted Slots: {slot_tags}\")\n",
    "    print()\n",
    "\n",
    "sentences = [\n",
    "    \"Add Jimmy Hendrix to my metal playlist\",\n",
    "    \"Book a table in the finest restaurant in Paris\",\n",
    "    \"I want to book a flight to New York tomorrow morning.\",\n",
    "    \"What's the weather in DC?\",\n",
    "    \"How is the weather in DC?\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    predict_intent_and_slots(\n",
    "        sentence, \n",
    "        model_intent, \n",
    "        model_slots, \n",
    "        tokenizer, \n",
    "        device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf2ec9-ebef-4da0-9f63-d124b7a57133",
   "metadata": {},
   "source": [
    "Nachdem wir zuvor zwei getrennte Modelle für die Vorhersage verwendet haben, wird dies im nächsten Abschnitt von einem Modell gemacht. Diese herangehensweise nennt sich 'Joint' Intent detection und Slot filling, da hier die beiden Vorhersagen vereint (joint) sind.\n",
    "Dafür wird wieder ein BERT Modell verwendet. Für diese Aufgabe bietet HuggingFace kein fertiges Modell an. Daher wird für dieses Beispiel die Implementierung eines Joint BERT Modells von [Jang Won Park](https://github.com/monologg) verwendet. Die Implementierung des Modells stammt aus seinem GitHub [Repository](https://github.com/monologg/JointBERT) [11] und basiert auf einem Paper von ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e27ad3-dedd-4610-9a44-576678cc50a7",
   "metadata": {},
   "source": [
    "Quellen:\n",
    "\n",
    "* 11: Jang Won Park, JointBERT, 2020, [Online: https://github.com/monologg/JointBERT, 25.06.2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2293026-072f-4e0b-ad79-c519d8cdfaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "try:\n",
    "    from torchcrf import CRF\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-crf\n",
    "    from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65bdd3b4-3266-4ef4-95d4-f2a1a927991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class SlotClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_slot_labels, dropout_rate=0.):\n",
    "        super(SlotClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_slot_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92fdbb1-8447-4029-b1ef-e3f3c831f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args, intent_label_lst, slot_label_lst):\n",
    "        super(JointBERT, self).__init__(config)\n",
    "        self.args = args\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args.dropout_rate)\n",
    "\n",
    "        if args.use_crf:\n",
    "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_labels_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        total_loss = 0\n",
    "        # 1. Intent Softmax\n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
    "            else:\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
    "            total_loss += intent_loss\n",
    "\n",
    "        # 2. Slot Softmax\n",
    "        if slot_labels_ids is not None:\n",
    "            if self.args.use_crf:\n",
    "                slot_loss = self.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
    "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "            else:\n",
    "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "                # Only keep active parts of the loss\n",
    "                if attention_mask is not None:\n",
    "                    active_loss = attention_mask.view(-1) == 1\n",
    "                    active_logits = slot_logits.view(-1, self.num_slot_labels)[active_loss]\n",
    "                    active_labels = slot_labels_ids.view(-1)[active_loss]\n",
    "                    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "                else:\n",
    "                    slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_labels_ids.view(-1))\n",
    "            total_loss += self.args.slot_loss_coef * slot_loss\n",
    "\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f2788-e6a8-457d-a7a2-7d289db00ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "joint_config = BertConfig.from_pretrained(model_name)\n",
    "model_joint = JointBERT.from_pretrained(model_name, config=joint_config, intent_label_lst=dsjnfnsdf, slot_label_lst=jdsifj)\n",
    "model_joint.to(device)\n",
    "\n",
    "training_args_joint = TrainingArguments(\n",
    "    output_dir=\"./joint_results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=None)\n",
    "\n",
    "trainer_joint = Trainer(\n",
    "    model=model_joint,\n",
    "    args=training_args_joint,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=, \n",
    "    eval_dataset=,\n",
    ")\n",
    "\n",
    "trainer_joint.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e3647-6347-4204-93e4-84a03fcbba93",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1902.10909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80078b75-1fb9-4126-8892-04b1cf7f86f5",
   "metadata": {},
   "source": [
    "## Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f665ef-9f05-49a1-93fa-9f6ed9c3551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
