{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e39565f-01fe-4d64-ad1a-83d69e597052",
   "metadata": {},
   "source": [
    "# Joint Intent detection and slot filling\n",
    "Dieses Jupyter notebook wurde als semesterabschließende Arbeit für das Modul Natural Language Processing an der [Fachhochschule Südwestfalen](https://www.fh-swf.de/en/international_3/index.php) erstellt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e89dc9-708f-4c4f-8371-36f810242e7d",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "Das Joint intent detection and slot filling (IDSF) ist eine Aufgabe aus dem Teilbereich des Natural Language Understandings (NLU) des Natural Language Processings (NLP), die uns in heutzutage fast täglich Alltag begegnet. Sei es um einen Timer auf dem Handy zu starten, bestimmte Musik abzuspielen oder das Licht einzuschalten. Der Ablauf ist dabei häufig der selbe: \"Siri stelle einen Timer für 4 Minuten\", \"Alexa spiele meine Schlager Playlist\" oder \"Google erstelle einen Arzttermin für heute 16:00 Uhr\". Meist beginnen die Kommandos mit dem Namen des Sprachassistenten, um diesen zu aktivieren, gefolgt vom Kommando für die gewünschte Aktion. Das IDSF beschäftigt sich dabei mit der Aufgabe, die gewünschte Aktion (Intent), also stelle einen Timer, Spiele Musik, erstelle einen Termin im Kalender und die dazugehörigen notwendigen Parameter, wie z.B. vier Minuten, Schlager Playlist oder Arzt heute 16:00 Uhr (Slots) zu erkennen.\n",
    "Da der Gebrauch dieser Sprachassistenten in Zukunft wahrscheinlich noch stärker zu nehmen wird, wollen wir uns deren funktionsweise in diesem Notebook näher anschauen. Dafür wird zuerst die Entwicklungshistorie vom IDSF betrachtet und anschließend wird ein eigenes Modell für die Erkennung erstellt und anhand eines selbst vorbereiteten Korpus trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d088a2-b7f1-41f3-bd3b-1858acb800e7",
   "metadata": {},
   "source": [
    "## Joint Intent Detection and Slot filling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bb627-85be-4f27-b800-9860a15cb12f",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f3c76-2bf2-49ff-9c87-6a1841c2c9ef",
   "metadata": {},
   "source": [
    "## Datenbeschaffung\n",
    "\n",
    "Als Datensatz für das nachfolgende Beispiel verwenden wir den Snips-Datensatz. Dieser Datensatz wurde vom, mittlerweil zu Sonos gehörenden [1], [Snips Team](https://snips.ai/) zusammengestellt, um ihr eigenes Modell mit anderen Wettbewerbern wie zum Beispiel Amazons Alexa zu verglichen. Die Ergebnisse und die Datensätze der drei Vergleiche wurden in einem [GitHub Repository](https://github.com/sonos/nlu-benchmark/tree/master) veröffentlicht und in dem Paper \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces\" [2] erläutert. Das Repository enthält die Daten für drei Evaluationen aus den Jahren 2016 bis 2018. Wir werden in diesem Notebook die Daten der 2017 durchgeführten Evaluation verwenden, da diese Sätze für sieben unterschiedliche und allgemeine Aufgaben enthält.\n",
    "\n",
    "Die Daten sind im dem Repository in einzelnen JSON-Dateien enthalten. Dabei gibt es pro Aufgabe zwei Dateien, eine für das Training und eine für die Validierung. Der Einfachheit halber wurden die Dateien in dem data Verzeichnis, dass diesem Notebook beiliegt, abgelegt.\n",
    "\n",
    "Nachfolgend ist ein Auszug aus der `train_AddToPlaylist_full.json`-Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b04077bd-4c1c-475b-bd64-997f7afd2ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"AddToPlaylist\": [\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"Add another \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"song\",\n",
      "          \"entity\": \"music_item\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" to the \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Cita Romántica\",\n",
      "          \"entity\": \"playlist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist. \"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"data\": [\n",
      "        {\n",
      "          \"text\": \"add \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"clem burke\",\n",
      "          \"entity\": \"artist\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" in \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"my\",\n",
      "          \"entity\": \"playlist_owner\"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \" playlist \"\n",
      "        },\n",
      "        {\n",
      "          \"text\": \"Pre-Party R&B Jams\",\n",
      "          \"entity\": \"playlist\"\n",
      "        }\n",
      "      ]\n",
      "    },\n"
     ]
    }
   ],
   "source": [
    "!head -n 48 data/train_AddToPlaylist_full.json # Zeige die ersten 23 Zeilen der angegebenen Datei an."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7977d9c-5cc6-4725-9386-46716911c4f5",
   "metadata": {},
   "source": [
    "Die Datei besteht an oberster Stelle aus dem Namen der gewünschten Aktion gefolgt von einer Liste an Objekten mit einem `Data` Attribut. Dieses enthält wiederum eine Liste von Objekten mit `Text` Attributen die den Satz in Teilen enthält. Dabei wird der Satz durch den Text eines definierten `Entities` geteilt. So enthält das erste Beispiel den Text bis zum ersten `entity` das als `music_item` klassifiziert wurde und wieder den gesamten Text bis zum nächsten entity, dem Namen einer Playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5966d-61dc-45af-8107-2fbf9f55b2de",
   "metadata": {},
   "source": [
    "Als nächstes werden die Daten in ein verwendbares Format transformiert. Ein in der Natural language processing gängiges Format ist das IOB Format. IOB steht für Inside-Outside-Beginning. Dieses Format ermöglicht die Kennzeichnung der einzelnen Entitäten in einem Satz. Es wird unteranderem von den weit verbreiteten Python Bibliotheken `NLTK` und `spaCy` unterstützt [3, 4]. Das Format wurde 1995 von Lance A. Ramshaw und Mitchell P. Marcus erfunden.\n",
    "\n",
    "Dieses Beispiel zeigt das Format einer Zeile, welches nachfolgend aus den JSON-Dateien erzeugt wird.\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o i-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "Am Beginn der Zeile steht der vollständige Satz abgetrennt durch ein BOS (begin of sentence) am Anfang des Satzes und ein EOS (end of sentence) am Ende des Satzes. Nun folgt das eigentliche IOB-Format. Dabei wird für jeden Token entweder der Buchstabe 'o', dieser steht für keine Bedeutung, der Buchstabe 'b', für den Beginn einer Entität die aus mehreren Token besteht, oder 'i', als Entität. Das 'i' steht dabei entweder nach einem 'b' wodurch eine Entität gekennzeichent wird, die aus mehreren Token besteht oder alleine für eine Entität die aus nur einem Token besteht.  Das 'b' und 'i' werden dabei jeweils gefolgt vom einem trennenden Bindestrich und der Entitätskategorie verwender. So ist der Name 'Clem Burke' unterteilt in ein `b-music_item` für Clem und `i-Music_item` für Burke. Dadurch wird definiert, dass die beiden Teile zusammen gehören.\n",
    "\n",
    "Das IOB2 Format ist eine Erweiterung des originalen IOB Formats. Es definiert das auch eine Entität die nur aus einem Token besteht mit einem 'b' kodiert wird und nicht wie im IOB Format mit einem 'i'. Dadurch ergibt sich das folgende Format:\n",
    "\n",
    "    BOS add clem burke in my playlist Pre-Party R&B Jams EOS o o b-mucic_item i-music_item o b-playlist_owner o b-playlist i-playlist i-playlist\n",
    "    \n",
    "\n",
    "Mit dem folgenden Python Code wird der Inhalt der im data-Verzeichnis liegenden Dateien in das vorgestellte Format transformiert. Die Dateien werden dabei im Verzeichnis `data/corpus` und einem Verzeichnis mit dem Titel der Intent-Kategorie abgelegt. Als Dateinamen werden `UUID`s verwendet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23b37ed-4ed4-4f10-b47f-d9856cac14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from shutil import rmtree\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049daa05-aa7a-4076-b36a-f4deb1ee1e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pfad data/corpus ist kein Verzeichnis oder existiert nicht\n",
      "Das Pfad data existiert nicht oder ist kein Verzeichnis.\n",
      "Try converting file at path data/validate_PlayMusic.json\n",
      "Finished converting file at path data/validate_PlayMusic.json. Writing to file...\n",
      "Try converting file at path data/train_SearchCreativeWork_full.json\n",
      "Finished converting file at path data/train_SearchCreativeWork_full.json. Writing to file...\n",
      "Try converting file at path data/train_AddToPlaylist_full.json\n",
      "Finished converting file at path data/train_AddToPlaylist_full.json. Writing to file...\n",
      "Try converting file at path data/train_RateBook_full.json\n",
      "Finished converting file at path data/train_RateBook_full.json. Writing to file...\n",
      "Try converting file at path data/train_SearchScreeningEvent_full.json\n",
      "Finished converting file at path data/train_SearchScreeningEvent_full.json. Writing to file...\n",
      "Try converting file at path data/validate_GetWeather.json\n",
      "Finished converting file at path data/validate_GetWeather.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchCreativeWork.json\n",
      "Finished converting file at path data/validate_SearchCreativeWork.json. Writing to file...\n",
      "Try converting file at path data/train_GetWeather_full.json\n",
      "Finished converting file at path data/train_GetWeather_full.json. Writing to file...\n",
      "Try converting file at path data/train_PlayMusic_full.json\n",
      "Finished converting file at path data/train_PlayMusic_full.json. Writing to file...\n",
      "Try converting file at path data/validate_RateBook.json\n",
      "Finished converting file at path data/validate_RateBook.json. Writing to file...\n",
      "Try converting file at path data/validate_AddToPlaylist.json\n",
      "Finished converting file at path data/validate_AddToPlaylist.json. Writing to file...\n",
      "Try converting file at path data/validate_BookRestaurant.json\n",
      "Finished converting file at path data/validate_BookRestaurant.json. Writing to file...\n",
      "Try converting file at path data/train_BookRestaurant_full.json\n",
      "Finished converting file at path data/train_BookRestaurant_full.json. Writing to file...\n",
      "Try converting file at path data/validate_SearchScreeningEvent.json\n",
      "Finished converting file at path data/validate_SearchScreeningEvent.json. Writing to file...\n",
      "Finished converting all files!\n"
     ]
    }
   ],
   "source": [
    "def clean_formatted(formatted_directory_path):\n",
    "    if not os.path.exists(formatted_directory_path) or not os.path.isdir(formatted_directory_path):\n",
    "        print(f'Pfad {formatted_directory_path} ist kein Verzeichnis oder existiert nicht')\n",
    "        return\n",
    "\n",
    "    rmtree(formatted_directory_path)\n",
    "    print('Alte Corpus-Dateien gelöscht')\n",
    "\n",
    "def convert_file(json_file_path):\n",
    "    print(f'Try converting file at path {json_file_path}')\n",
    "    if not os.path.isfile(json_file_path):\n",
    "        print(f'File {file_path} does not exists', json_file_path)\n",
    "        return \n",
    "    formatted_lines = []\n",
    "    intent_category = None\n",
    "    with open(json_file_path, 'r', encoding='latin-1') as json_file:\n",
    "        json_content = json.load(json_file)\n",
    "        intent_category = next(iter(json_content))\n",
    "        for sentence_block in json_content[intent_category]:\n",
    "            sentence = \"\"\n",
    "            slots = []\n",
    "            for sentence_data_block in sentence_block['data']:\n",
    "                sentence_part = sentence_data_block['text']\n",
    "                sentence_part = re.sub('\\n', '', sentence_part)\n",
    "                if sentence_part != '':\n",
    "                    sentence += sentence_part\n",
    "                sentence_part_len = len(sentence_part.split())\n",
    "                if 'entity' in sentence_data_block:\n",
    "                    entity_type = sentence_data_block['entity']\n",
    "                    if sentence_part_len > 1:\n",
    "                        firstSlot = True\n",
    "                        for i in range(sentence_part_len):\n",
    "                            if firstSlot:\n",
    "                                slots.append('b-' + entity_type)\n",
    "                                firstSlot = False\n",
    "                            else:\n",
    "                                slots.append('i-' + entity_type)\n",
    "                    else:\n",
    "                        slots.append('b-' + entity_type)\n",
    "                else:\n",
    "                    for i in range(sentence_part_len):\n",
    "                        slots.append('o')\n",
    "            formatted_lines.append(construct_row(sentence, intent_category, slots))\n",
    "    print(f'Finished converting file at path {json_file_path}. Writing to file...')\n",
    "    write_to_file(intent_category, formatted_lines)\n",
    "    \n",
    "\n",
    "def construct_row(sentence, intent, slots):\n",
    "    row = ''\n",
    "    row += sentence\n",
    "    row += '#!#'\n",
    "    row += intent\n",
    "    row += '#!#'\n",
    "    row += ' '.join(slots)\n",
    "    row += '\\n'\n",
    "    return row\n",
    "\n",
    "\n",
    "def write_to_file(intent, lines):\n",
    "    if intent is None or intent == '':\n",
    "        print('No intent')\n",
    "        return\n",
    "        \n",
    "    base_output_directory = 'data/corpus/'\n",
    "    output_file_path = base_output_directory + intent + '/' + str(uuid.uuid4()) + '.txt'\n",
    "\n",
    "    if not os.path.exists(base_output_directory + intent): \n",
    "        os.makedirs(base_output_directory + intent)\n",
    "    \n",
    "    with open(output_file_path, 'a') as output_file:\n",
    "        output_file.writelines(lines)\n",
    "\n",
    "\n",
    "def iterate_over_json_files_in_directory(directory_path):\n",
    "    if not os.path.exists(directory_path) or directory_path is not os.path.isdir(directory_path):\n",
    "        print(f\"Das Pfad {directory_path} existiert nicht oder ist kein Verzeichnis.\")\n",
    "        \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):  # Nur JSON-Dateien berücksichtigen\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            convert_file(file_path)\n",
    "    print('Finished converting all files!')\n",
    "    \n",
    "clean_formatted('data/corpus')\n",
    "iterate_over_json_files_in_directory('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dd92-a1b1-404d-b1af-e13fa5ebbfa4",
   "metadata": {},
   "source": [
    "Als nächstes wird geprüft, ob auch alle Einträge in der Textdatei enthalten sind. Dafür werden die Einträge in der train- und validate.json mit der Anzahl der Zeilen in der Textdatei verglichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99108436-e37e-4954-be8d-66a2e8104be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2056 data/formatted/RateBook.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "579d5d3c-fc2a-490c-98fe-c98bf612674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m1956\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/train_RateBook_full.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431915fc-2450-4617-aad4-37ecd6e94991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;39m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jq '.RateBook | length' data/validate_RateBook.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490f728-7789-4273-902a-8f9f11235122",
   "metadata": {},
   "source": [
    "Man sieht, dass die Anzahl der `data`-Blöcke aus den JSON-Dateien der Anzahl der Zeilen in der erzeugten Textdatei entspricht. Die Konvertierung war also erfolgreich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d32f47ef-983c-4ba5-a359-a808e2cf2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS rate The Lotus and the Storm zero of 6 EOS RateBook o b-object_name i-object_name i-object_name i-object_name i-object_name b-rating_value o b-best_rating\n",
      "BOS Rate The Fall-Down Artist 5 stars. EOS RateBook o b-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS Rate the current novel one points EOS RateBook o o b-object_select b-object_type b-rating_value b-rating_unit\n",
      "BOS rate The Ape-Man Within 4 EOS RateBook o b-object_name i-object_name i-object_name b-rating_value\n",
      "BOS I give The Penalty three stars EOS RateBook o o b-object_name i-object_name b-rating_value b-rating_unit\n",
      "BOS rate this novel a 4 EOS RateBook o b-object_select b-object_type o b-rating_value\n",
      "BOS give 5 out of 6 points to Absolutely, Positively Not series EOS RateBook o b-rating_value o o b-best_rating b-rating_unit o b-object_name i-object_name i-object_name b-object_part_of_series_type\n",
      "BOS I give Emile, or On Education five points. EOS RateBook o o b-object_name i-object_name i-object_name i-object_name b-rating_value b-rating_unit o\n",
      "BOS rate Licence Renewed a 4 EOS RateBook o b-object_name i-object_name o b-rating_value\n",
      "BOS Give this essay a 2 out of 6. EOS RateBook o b-object_select b-object_type o b-rating_value o o b-best_rating o\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 data/formatted/RateBook.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9ff63-9acc-462e-b20b-c39fcb78caf4",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 1: https://investors.sonos.com/news-and-events/investor-news/latest-news/2019/Sonos-Announces-Acquisition-of-Snips/default.aspx, [Online, 07.03.2025]\n",
    "* 2: Coucke A. et al., \"Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces.\" 2018, [Online: https://arxiv.org/abs/1805.10190, 07.03.2025]\n",
    "* 3: NLTK Team, tree2conlltags, [Online: https://www.nltk.org/_modules/nltk/chunk/util.html#tree2conlltags, 13.03.2025]\n",
    "* 4: Explosion, spaCy convert, [Online: https://spacy.io/api/cli#converters, 13.03.2025]\n",
    "* 5: Ramshaw und Marcus, \"Text Chunking using Transformation-Based Learning\" 1995, [Online: https://arxiv.org/abs/cmp-lg/9505040, 14.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511baabd-4337-4292-9483-482fd44392e8",
   "metadata": {},
   "source": [
    "## Erstellen eines Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc2109-e2af-4849-a245-6867a42bea03",
   "metadata": {},
   "source": [
    "Nachdem nun die Daten in ein verwendbares Format tranformiert wurden, müssen wir den effiziente Zugriff auf die Daten sicherstellen. Dafür bietet die Python-Bibliothek NLTK verschiedene `CorpusReader` Klassen zur Verfügung. Diese Klassen ermöglichen über Methoden den Zugriff auf die Dokumente selbst, sowie auf weitere Dateien eines Corpus, wie zum Beispiel die Lizenz des Corpus oder eine README.md Datei. NLTK bietet eine große Anzahl an CorpusReadern für verschiedenste Szenarien an. Ein einfacher CorpusReader ist der `PlainTextCorpusReader` [7]. Dieser bietet Zugriff auf reine Textdateien. Eine vollständige Liste ist in der [Dokumentation](https://www.nltk.org/api/nltk.corpus.reader.html) von NLTK zu finden.\n",
    "\n",
    "Für das zuvor definierte Format gibt es keinen passenden vorgefertigten Reader. Daher müssen wir einen eigenen erstellen. Dafür kann die `CorpusReader`-Basisklasse [8] erweitert werden oder ein bestehender verwendet werden. Wir werden den `CategorizedPlaintextCorpusReader` als Basis verwenden. Dieser ist eine Erweiterung des zuvor erwähnten `PlainTextCorpusReader`, welcher Zugriff auf Textdateien bietet.  Durch die Nutzung des `CategorizedPlaintextCorpusReader` können die Dateien zusätzlich in Kategorien unterteilt werden. Dies geschieht über reguläre Ausdrücke. Im nachfolgenden Code definiert der reguläre Ausdruck _category_pattern_ die Kategorie als den Namen des Verzeichnisses, indem die Textdateien liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c56201-542f-4d46-8644-27e5da9e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import CategorizedPlaintextCorpusReader\n",
    "from nltk import wordpunct_tokenize\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe06789a-d306-4896-a9f9-0640f8d0d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOBCorpusReader(CategorizedPlaintextCorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids, cat_pattern, encoding='utf-8'):\n",
    "        super().__init__(root, fileids, cat_pattern=cat_pattern, encoding=encoding)\n",
    "\n",
    "    def resolve(self, fileids=None, categories=None):\n",
    "\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError('Specify only one')\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        else:\n",
    "            return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with codecs.open(path, 'r', encoding=encoding) as file:\n",
    "                yield file.read()\n",
    "        \n",
    "    def lines(self, fileids=None, categories=None):\n",
    "\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for line in doc.split('\\n'):\n",
    "                yield line\n",
    "\n",
    "    def line_parts(self, fileids=None, categories=None):\n",
    "\n",
    "        for line in self.lines(fileids, categories):\n",
    "            if line == '':\n",
    "                continue\n",
    "            yield line.split('#!#')\n",
    "\n",
    "    def intents(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, labels in self.line_parts(fileids, categories):\n",
    "            yield (sentence, intent)\n",
    "\n",
    "    def slots(self, fileids=None, categories=None):\n",
    "\n",
    "        for sentence, intent, slots in self.line_parts(fileids, categories):\n",
    "            yield (sentence, slots.split())\n",
    "\n",
    "    def joint_data(self, fileids=None, categories=None):\n",
    "\n",
    "        for (sentence, intent), (sentence, slots) in zip(self.intents(fileids, categories), self.slots(fileids, categories)):\n",
    "            yield (sentence, intent, slots)\n",
    "\n",
    "\n",
    "    \n",
    "file_pattern = r'.*\\.txt'\n",
    "category_pattern = r'([^/]+)/[^/]+\\.txt$'\n",
    "corpus = IOBCorpusReader('data/corpus', file_pattern, cat_pattern=category_pattern)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe06743-f036-4612-8817-ef65f0714b1f",
   "metadata": {},
   "source": [
    "Quellen\n",
    "\n",
    "* 6: Bengfort, Benjamin, et al. Applied Text Analysis with Python : Enabling Language-Aware Data Products with Machine Learning, O'Reilly Media, Incorporated, 2018. ProQuest Ebook Central, https://ebookcentral.proquest.com/lib/fh-swf/detail.action?docID=5425029.\n",
    "* 7: NLTK Project, PlainTextCorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader.plaintext.html#nltk.corpus.reader.plaintext.PlaintextCorpusReader, 17.03.2025]\n",
    "* 8: NLTK project, CorpusReader, 2024, [Online: https://www.nltk.org/api/nltk.corpus.reader#nltk.corpus.reader.CorpusReader, 17.03.2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24959113-27fe-480f-8dd0-19719694440b",
   "metadata": {},
   "source": [
    "## Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb037c-df46-409b-8d55-2a25fcf727aa",
   "metadata": {},
   "source": [
    "Beschreibung von BERT !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa01c88b-b23e-4e00-87a6-87106383e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8e0638-793c-418a-9e74-a202212344ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_label_to_id = defaultdict(lambda: len(intent_label_to_id))\n",
    "slot_label_to_id = defaultdict(lambda: len(slot_label_to_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f33973-79ef-4b16-9200-93e0d2d1b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Tokenizer von Hugging Face laden \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Daten aus Corpus laden\n",
    "joint_dataset = list(corpus.joint_data(categories='AddToPlaylist'))\n",
    "\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "intent_labels_list = []\n",
    "slot_labels_list = []\n",
    "\n",
    "for data in joint_dataset:\n",
    "    # Tokenisierung des Satzes mit dme BERT-Tokenizer\n",
    "    encoding = tokenizer(data[0], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\", is_split_into_words=True)\n",
    "\n",
    "    # IOB-Tags an Subtoken-Länge anpassen\n",
    "    iob_tags = data[2]\n",
    "    aligned_slot_labels = []\n",
    "    for word, label in zip(data[0], iob_tags):\n",
    "        sub_tokens = tokenizer.tokenize(word)\n",
    "        aligned_slot_labels.extend([label] * len(sub_tokens))\n",
    "    \n",
    "    # Abschneiden oder Auffüllen der Slot-Labels\n",
    "    aligned_slot_labels = aligned_slot_labels[:128] + [\"o\"] * (128 - len(aligned_slot_labels))\n",
    "\n",
    "    # Konvertieren in numerische Repräsentationen\n",
    "    slot_label_ids = [slot_label_to_id[label] for label in aligned_slot_labels]\n",
    "    intent_label_id = intent_label_to_id[data[1]]\n",
    "\n",
    "    # Tensoren zur Liste hinzufügen\n",
    "    input_ids_list.append(encoding[\"input_ids\"].squeeze(0))  # Entfernt die Batch-Dimension\n",
    "    attention_masks_list.append(encoding[\"attention_mask\"].squeeze(0))  # Entfernt die Batch-Dimension\n",
    "    slot_labels_list.append(torch.tensor(slot_label_ids, dtype=torch.long))  # Konvertieren in Tensor\n",
    "    intent_labels_list.append(intent_label_id)\n",
    "\n",
    "# Listen in einzelne Tensoren umwandeln\n",
    "input_ids = torch.stack(input_ids_list)  # [batch_size, seq_length]\n",
    "attention_masks = torch.stack(attention_masks_list)  # [batch_size, seq_length]\n",
    "slot_labels = torch.stack(slot_labels_list)  # [batch_size, seq_length]\n",
    "intent_labels = torch.tensor(intent_labels_list, dtype=torch.long)  # [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8758cc20-a3e0-43bc-9f14-d0a8d39da26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_103/1346844624.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = intent_model(input_ids=torch.tensor(input_ids), attention_mask=torch.tensor(attention_masks), labels=torch.tensor(intent_labels))\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Intent Detection Modell\n",
    "intent_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(intent_label_to_id))\n",
    "\n",
    "# Training vorbereiten\n",
    "outputs = intent_model(input_ids=torch.tensor(input_ids), attention_mask=torch.tensor(attention_masks), labels=torch.tensor(intent_labels))\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bf7a67d-d4a3-40bf-a357-ed9e4ef413b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m slot_model \u001b[38;5;241m=\u001b[39m BertForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(slot_label_to_id))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Training vorbereiten\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m slot_model(input_ids\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m, attention_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(attention_masks), labels\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(slot_labels))\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# Slot Filling Modell\n",
    "slot_model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(slot_label_to_id))\n",
    "\n",
    "# Training vorbereiten\n",
    "outputs = slot_model(input_ids=torch.cat(input_ids, dim=0), attention_mask=torch.cat(attention_masks, dim=0), labels=torch.tensor(slot_labels))\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7483b7-e583-431e-b2c0-e6828a09cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class JointIntentSlotModel(nn.Module):\n",
    "    def __init__(self, num_intents, num_slots):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Linear(self.bert.config.hidden_size, num_slots)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state  # Für Slots\n",
    "        pooled_output = outputs.pooler_output       # Für Intents\n",
    "\n",
    "        # Intents und Slots vorhersagen\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        \n",
    "        return intent_logits, slot_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94501919-62b1-4664-aa0f-d29c217cd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verlustfunktionen\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "slot_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Modell-Output\n",
    "model = JointIntentSlotModel(num_intents=len(intent_label_to_id), num_slots=len(slot_label_to_id))\n",
    "intent_logits, slot_logits = model(torch.tensor(input_ids), torch.tensor(attention_masks))\n",
    "\n",
    "# Berechnung des Verlusts\n",
    "intent_loss = intent_loss_fn(intent_logits, torch.tensor(intent_labels))\n",
    "slot_loss = slot_loss_fn(slot_logits.view(-1, len(slot_label_to_id)), torch.tensor(slot_labels).view(-1))\n",
    "\n",
    "# Gesamtverlust\n",
    "loss = intent_loss + slot_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031d3d2-6486-47c1-94a6-7c4e3be6053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Intent-Evaluierung\n",
    "intent_preds = torch.argmax(intent_logits, dim=1).numpy()\n",
    "print(classification_report(intent_labels, intent_preds, target_names=intent_label_to_id.keys()))\n",
    "\n",
    "# Slot-Evaluierung\n",
    "slot_preds = torch.argmax(slot_logits, dim=2).numpy()\n",
    "for true, pred in zip(slot_labels, slot_preds):\n",
    "    print(\"True:\", [id_to_slot_label[i] for i in true])\n",
    "    print(\"Pred:\", [id_to_slot_label[i] for i in pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80078b75-1fb9-4126-8892-04b1cf7f86f5",
   "metadata": {},
   "source": [
    "## Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f665ef-9f05-49a1-93fa-9f6ed9c3551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
